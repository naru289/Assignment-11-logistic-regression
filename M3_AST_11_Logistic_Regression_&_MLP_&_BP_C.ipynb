{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-11-logistic-regression/blob/main/M3_AST_11_Logistic_Regression_%26_MLP_%26_BP_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 11 : Logistic Regression and Multi-Layer Perceptron using Keras and Implementation of Back Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "   \n",
        "  At the end of the experiment, you will be able to :\n",
        "    \n",
        "  * implement Logistic Regression using python code and sklearn library\n",
        "  * understand the concept of Multi Layer Perceptron (MLP)\n",
        "  * build an image classifier using the Keras Sequential API\n",
        "  * understand the backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAsQn9McNyVD"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The dataset named \"Heart Disease Dataset\" comes from a study conducted in 1988 and originates from the UCI Machine Learning Repository. The task is to get the best predictor and guess if a patient has a heart disease.\n",
        "\n",
        "The dataset consists of 303 individuals data. There are 14 columns in the dataset, which are described below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "503VK75SUE11"
      },
      "source": [
        "**1. Age:** The person’s age in years\n",
        "\n",
        "**2. Sex:** The person’s Gender (1 = male, 0 = female)\n",
        "\n",
        "**3. cp - chest pain type:** The type of chest pain experienced by the individual person\n",
        "\n",
        "\n",
        "\n",
        "*  0: typical angina \n",
        "\n",
        "*  1: atypical angina\n",
        "\n",
        "*  2: non-anginal pain\n",
        "\n",
        "*  3: asymptotic\n",
        "\n",
        "\n",
        "**4. trestbps - Resting Blood Pressure:** The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
        "\n",
        "**5. chol - Serum Cholestrol:** The person’s cholesterol measurement in mg/dl\n",
        "\n",
        "**6. fbs - Fasting Blood Sugar:** The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
        "\n",
        "**7. restecg - Resting ECG:** resting electrocardiographic results\n",
        "\n",
        "*   0: normal\n",
        "*   1: having ST-T wave abnormality\n",
        "*   2: left ventricular hyperthrophy\n",
        "\n",
        "\n",
        "**8. thalach - Max heart rate achieved:** The person’s maximum heart rate achieved\n",
        "\n",
        "**9. exang - Exercise induced angina:** Exercise induced angina (1 = yes; 0 = no)\n",
        "\n",
        "**10. oldpeak - ST depression induced by exercise relative to rest:** ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot.)\n",
        "\n",
        "**11. slope - Peak exercise ST segment:** The slope of the peak exercise ST segment \n",
        "\n",
        "\n",
        "*  0: downsloping\n",
        "*  1: flat\n",
        "*  2: upsloping\n",
        "\n",
        "\n",
        "**12. ca - Number of major vessels (0–3) colored by flourosopy:** The number of major vessels (0–3)\n",
        "\n",
        "**13. thal:** A blood disorder called thalassemia \n",
        "\n",
        "\n",
        "*   0: NULL (dropped from the dataset)\n",
        "*   1: fixed defect (no blood flow in some part of the heart)\n",
        "*   2: normal blood flow\n",
        "*   3: reversible defect (a blood flow is observed but it is not normal)\n",
        "\n",
        "**14. target:** Heart disease (1 = no, 0= yes)\n",
        "\n",
        "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/statlog+(heart)\n",
        "\n",
        "\n",
        "\n",
        "**Problem description:**\n",
        "\n",
        "The goal is to predict the binary class Heart Disease (target), which represents whether or not a patient has heart disease:\n",
        "\n",
        "0 represents no heart disease present\n",
        "\n",
        "1 represents heart disease present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_11_Logistic_Regression_&_MLP_&_BP_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Heart_Disease.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1wljNTNvmIr"
      },
      "source": [
        "### Importing the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TaICL_cI8yX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LH0pQnKI8yb"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5pC_cikm-Ai"
      },
      "source": [
        "df = pd.read_csv('Heart_Disease.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-lmLLgFz2m5"
      },
      "source": [
        "# Check for the shape of the dataset\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQD56JkD15MK"
      },
      "source": [
        "Check for the duplicate rows in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XSvL0SM1YD2"
      },
      "source": [
        "df[df.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KCcxaNuzs0c"
      },
      "source": [
        "# Drop the duplicated row from the dataset\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp2Jwb5nz6hc"
      },
      "source": [
        "# Check for the shape after removing the duplicated row\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_p3v_-9zfH6"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-H8ueX2hR3"
      },
      "source": [
        "The info() function is used to print a concise summary of a Data Frame. This method prints information about a Data Frame including the index dtype and column dtypes, non-null values, and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Siwajp3x2I0_"
      },
      "source": [
        "# summarizes the count, mean, standard deviation, min, and max for numeric variables.\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apDG23Jt2qEW"
      },
      "source": [
        "The describe() function computes a summary of statistics pertaining to the Data Frame columns. This function gives the mean and std values. Also, the function excludes the character columns and gives a summary about numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBy-GjVoAAhS"
      },
      "source": [
        "# Check for missing values\n",
        "print(df.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxcKaiCh2M_J"
      },
      "source": [
        "There are no missing values in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see if there is a good proportion between our positive & negative binary predictor."
      ],
      "metadata": {
        "id": "5JNGsxFuLIip"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHaDLvw-Utq"
      },
      "source": [
        "# Check for the target values\n",
        "df['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a good balance between the two binary outputs\n"
      ],
      "metadata": {
        "id": "zQ-atr2aLObF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHPVjjP23Kqy"
      },
      "source": [
        "Countplot shows the counts of observations in each categorical bin using bars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lw6hjpx-8EM"
      },
      "source": [
        "# Visualization of the target column\n",
        "sns.countplot(x=df['target']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJgtnH9F_KPS"
      },
      "source": [
        "Here “1” represents the number of people suffering from heart disease and “0” represents the number of people who are not suffering from heart disease. Hence, the total number of people suffering from heart disease is “165” and the number of people not suffering from heart disease is “138”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd00lXpx-RSN"
      },
      "source": [
        "## Exploratory Data Analysis \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of Heart Disease and No Heart Disease\n",
        "countNoDisease = len(df[df.target == 0])\n",
        "countHaveDisease = len(df[df.target == 1])\n",
        "print(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease / (len(df.target))*100)))\n",
        "print(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease / (len(df.target))*100)))"
      ],
      "metadata": {
        "id": "MxoGo_dVXGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlations\n",
        "\n",
        "Find the **correlation matrix** among all the feature variables and check whether the features are positively or negatively correlated with our predictor (target)."
      ],
      "metadata": {
        "id": "E0HNEceULxnO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ODP4J8fcX5S"
      },
      "source": [
        "# Calculate correlation matrix\n",
        "plt.subplots(figsize=(20,10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe positive correlation between `target` and `cp`,  the greater amount of chest pain results in a greater chance of having heart disease and also with`thalach`, `slope` \n",
        "\n",
        "we see a negative correlation between exercise induced angina (exang) & our predictor `target`, because when you excercise, your heart requires more blood, but narrowed arteries slow down blood flow and also with `sex`, `exang`, `ca`, `thai`, `oldpeak`"
      ],
      "metadata": {
        "id": "5efTVKo5OeOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(10, 5), \n",
        "                                                        title=\"Correlation with the target feature\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "cTOM7DfQcYLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the above graph are:\n",
        "\n",
        "Four feature( `“cp”, “restecg”, “thalach”, “slope”` ) are positively correlated with the target feature. Other features are negatively correlated with the target feature.\n"
      ],
      "metadata": {
        "id": "z69G8gA1c8tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize **Pairplots** with continuous features"
      ],
      "metadata": {
        "id": "SbTGMppqQWpk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px3p_pGq_MPi"
      },
      "source": [
        "subData = df[['age','trestbps','chol','thalach','oldpeak']]\n",
        "sns.pairplot(subData);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the **histogram** to see the distribution of each column in the data\n"
      ],
      "metadata": {
        "id": "lGc6V124Ro5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(figsize=(12,12), layout=(5,3));"
      ],
      "metadata": {
        "id": "dKLwK191RtyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot with Histograms we can see the shape of each feature and provides the count of number of observations in each bin.\n"
      ],
      "metadata": {
        "id": "y8S5YD27R-qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualze the features and their relation with the target(Heart Disease or No Heart Disease)"
      ],
      "metadata": {
        "id": "wafmZb11ShaN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqf5T0hvFbGS"
      },
      "source": [
        "**\"Age\" Feature Analysis**\n",
        "\n",
        "Let us look at the people’s age who are suffering from the heart disease or not.\n",
        "Here, target = 1 implies that the person is suffering from heart disease and target = 0 implies the person is not suffering.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzfpfifoAMX-"
      },
      "source": [
        "pd.crosstab(df.age, df.target).plot(kind=\"bar\",figsize=(20,6))\n",
        "plt.title('Heart Disease Frequency for Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTzLGbUeFkRi"
      },
      "source": [
        "We see that most people who are suffering are of the age of 58, followed by 57.\n",
        "Majorly, people belonging to the age group 50+ are suffering from the disease."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sex(“sex”) Feature Analysis**"
      ],
      "metadata": {
        "id": "OA_rOerxhvjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x=df['sex'])\n",
        "plt.xlabel(\"Sex (0 = female, 1= male)\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "0SEgjfLxhuVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it is clearly visible that, Ratio of Male to Female is approx 2:1"
      ],
      "metadata": {
        "id": "kOxpgsgriEPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of male and female patients\n",
        "countFemale = len(df[df.sex == 0])\n",
        "countMale = len(df[df.sex == 1])\n",
        "print(\"Percentage of Female Patients: {:.2f}%\".format((countFemale / (len(df.sex))*100)))\n",
        "print(\"Percentage of Male Patients: {:.2f}%\".format((countMale / (len(df.sex))*100)))"
      ],
      "metadata": {
        "id": "KQA-96CPZAwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(8,6),color=['#1CA53B','#AA1111'])\n",
        "plt.title('Heart Disease Frequency for Sex')\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend([\"Haven't Disease\", \"Have Disease\"])\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IhfeibidYuBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can observe that the heart disease risk is high in Male compared to Female"
      ],
      "metadata": {
        "id": "ZumRqpKAZLIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s plot the relation between `sex` and `slope`.\n",
        "\n"
      ],
      "metadata": {
        "id": "aBXosgk5iH_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x=df[\"sex\"], hue=df[\"slope\"])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "lYbQxh_niJba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can observe that the slope value is higher in the case of males(1)."
      ],
      "metadata": {
        "id": "YI2pX_hujxjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chest Pain Type(“cp”) Feature Analysis**"
      ],
      "metadata": {
        "id": "bR937dx-j8F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,6))\n",
        "sns.countplot(x=df['cp'])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "ijsjyRYkkBTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. status at least\n",
        "1. condition slightly distressed\n",
        "2. condition medium problem\n",
        "3. condition too bad"
      ],
      "metadata": {
        "id": "QgpV_ddAlmZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing cp vs target column**\n"
      ],
      "metadata": {
        "id": "P98Yyh0zkvc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x=df['cp'], hue=df[\"target\"])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "r-QNNlhOktx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can observe that,\n",
        "\n",
        "* People having the least chest pain are not likely to have heart disease.\n",
        "\n",
        "* People having severe chest pain are likely to have heart disease.\n",
        "Elderly people are more likely to have chest pain."
      ],
      "metadata": {
        "id": "B13MgeS5lPh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing slope vs Heart Disease**"
      ],
      "metadata": {
        "id": "USkAo9oTmAet"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeAWeRxjKfv"
      },
      "source": [
        "sns.catplot(x=\"target\", y=\"oldpeak\", hue=\"slope\", kind=\"bar\", data=df);\n",
        "\n",
        "plt.title('ST depression (induced by exercise relative to rest) vs. Heart Disease', size=15)\n",
        "plt.xlabel('Heart Disease',size=20)\n",
        "plt.ylabel('ST depression',size=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ST segment depression occurs because when the ventricle is at rest and therefore repolarized. If the trace in the `ST` segment is abnormally low below the baseline, this can lead to this Heart Disease. This supports the plot above because low ST Depression yields people at greater risk for heart disease. While a high ST depression is considered normal & healthy. The `“slope”` hue, refers to the peak exercise ST segment, with values: 0: upsloping , 1: flat , 2: downsloping). Both positive & negative heart disease patients exhibit equal distributions of the 3 slope categories."
      ],
      "metadata": {
        "id": "MPw_sOkDTpE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Violin & Box Plots\n",
        "\n",
        "The advantages of showing the Box & Violin plots is that it shows the basic statistics of the data, as well as its distribution. These plots are often used to compare the distribution of a given variable across some categories.\n",
        "\n",
        "It shows the median, IQR, & Tukey’s fence. (minimum, first quartile (Q1), median, third quartile (Q3), and maximum).\n",
        "\n",
        "In addition it can provide us with outliers in our data."
      ],
      "metadata": {
        "id": "Ca5l99LOUTJO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd8HcEle3mC6"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.violinplot(x='target', y='oldpeak', hue=\"sex\", inner='quartile', data= df)\n",
        "plt.title(\"Thalach Level vs. Heart Disease\",fontsize=20)\n",
        "plt.xlabel(\"Heart Disease Target\", fontsize=16)\n",
        "plt.ylabel(\"Thalach Level\", fontsize=16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we can see that the overall shape & distribution for negative & positive patients differ vastly. Positive patients exhibit a lower median for ST depression level & thus a great distribution of their data is between 0 & 2, while negative patients are between 1 & 3. In addition, we don’t see many differences between male & female target outcomes."
      ],
      "metadata": {
        "id": "6agA9lYLUuZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(x='target', y='thalach', hue=\"sex\", data=df)\n",
        "plt.title(\"ST depression Level vs. Heart Disease\", fontsize=20)\n",
        "plt.xlabel(\"Heart Disease Target\",fontsize=16)\n",
        "plt.ylabel(\"ST depression induced by exercise relative to rest\", fontsize=15);"
      ],
      "metadata": {
        "id": "iboWw8dYU8tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive patients exhibit a heightened median for ST depression level, while negative patients have lower levels. In addition, we don’t see many differences between male & female target outcomes, expect for the fact that males have slightly larger ranges of ST Depression."
      ],
      "metadata": {
        "id": "juYVYGkWVPeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the categorical data\n",
        "\n",
        "scikit-learn also supports one hot encoding via LabelBinarizer and OneHotEncoder in its preprocessing module. \n",
        "\n",
        "First, let’s understand the types of `categorical data`:\n",
        "\n",
        "First, let’s understand the types of categorical data:\n",
        "1. **Nominal Data:** The nominal data called labelled/named data. Allowed to change the order of categories, change in order doesn’t affect its value. For example, Gender (Male/Female/Other), Age Groups (Young/Adult/Old), etc.\n",
        "2. **Ordinal Data:** Represent discretely and ordered units. Same as nominal data but have ordered/rank. Not allowed to change the order of categories. For example, Ranks: 1st/2nd/3rd, Education: (High School/Undergrads/Postgrads/Doctorate), etc.\n",
        "\n",
        "Since `‘cp’`, `‘thal’`, `'restecg'` and `‘slope’` are categorical variables (Nominal attributes) we’ll perform one hot encoding on these variables.\n",
        "\n",
        "**Note:** Refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to understand one-hot encoding from sklearn\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfMFRF72cPgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# put categorical variables in a list \n",
        "categorical_vars = ['cp', 'thal', 'restecg', 'slope']\n",
        "\n",
        "# instantiate the one hot encoder\n",
        "# Will return sparse matrix if set True else will return an array.\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)"
      ],
      "metadata": {
        "id": "3cT43UtsljZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the one hot encoder \n",
        "encoder_vars_array = one_hot_encoder.fit_transform(df[categorical_vars]).astype(int)\n",
        "\n",
        "# create object for the feature names using the categorical variables\n",
        "encoder_feature_names = one_hot_encoder.get_feature_names_out(categorical_vars)\n",
        "\n",
        "# create a dataframe to hold the one hot encoded variables\n",
        "encoder_vars_df = pd.DataFrame(encoder_vars_array, columns = encoder_feature_names)\n",
        "\n",
        "# concatenate the new dataframe back to the original input variables dataframe\n",
        "data_encoded = pd.concat([df.reset_index(drop=True), encoder_vars_df.reset_index(drop=True)], axis = 1)\n",
        "\n",
        "# drop the original categorical variables\n",
        "data_encoded.drop(categorical_vars, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "W95rdfmzl91V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_encoded.head()"
      ],
      "metadata": {
        "id": "rOHOWIQTw8NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jL5b5_kX5nf"
      },
      "source": [
        "### Storing the features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clR9VMVlXtPh"
      },
      "source": [
        "X = data_encoded.drop('target', axis = 1) # Features\n",
        "y = data_encoded['target'] # Label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd22mtSp9gTc"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D5l0XFXYQLh"
      },
      "source": [
        "### Normalization of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzhpdBRaxPLv"
      },
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV86WxDp9dJP"
      },
      "source": [
        "X[[\"age\"]] = scaler.fit_transform(X[[\"age\"]])\n",
        "X[[\"trestbps\"]] = scaler.fit_transform(X[[\"trestbps\"]])\n",
        "X[[\"chol\"]] = scaler.fit_transform(X[[\"chol\"]])\n",
        "X[[\"thalach\"]] = scaler.fit_transform(X[[\"thalach\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p49qG0mlYFsz"
      },
      "source": [
        "### Splitting the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AGWCsnYHzf"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWcQyhbCxmX"
      },
      "source": [
        "### Logistic Regression from scratch using gradient method \n",
        "\n",
        "#### What is Logistic Regression?\n",
        "\n",
        "* Logistic Regression is a Supervised statistical technique to find the probability of dependent variable(Classes present in the variable).\n",
        "\n",
        "* Logistic regression uses function called the logit function,that helps derive a relationship between the dependent variable and independent variables by predicting the probabilities or chances of occurrence.\n",
        "\n",
        "* The logistic function (also known as the sigmoid function) convert the probabilities into binary values which could be further used for predictions.\n",
        "\n",
        "#### Why Logistic, not Linear?\n",
        "\n",
        "With binary classification, let ‘X’ be some feature and ‘y’ be the output which can be either 0 or 1.\n",
        "\n",
        "\n",
        "For Linear Regression, we had the hypothesis $\\hat{y} = w.X + b $ , whose output range was the set of all Real Numbers.\n",
        "\n",
        "Now, for Logistic Regression our hypothesis is  $h_{\\theta}(X) = sigmoid(w.X +b)$ , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.  \n",
        "\n",
        "$h_{\\theta}(X) = \\frac{1}{1 +e^{-(w.X + b)}}$\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/logistic.png\" width= 400px/>\n",
        "</center>\n",
        "\n",
        "The probability that the output is 1 given its input can be represented as:\n",
        "\n",
        "$ P(y = 1 \\ | \\ x)$\n",
        "\n",
        "\n",
        "In the formula of the logistic model,\n",
        "\n",
        "when $ w.X + b == 0 $, then the probability (p) will be 0.5,\n",
        "\n",
        "similarly, $ w.X + b > 0 $, then the p will be going towards 1 and $ w.X + b < 0 $, then the probability (p) will be going towards 0.\n",
        "\n",
        "Interpretation of the weights differ from the Linear Regression as the output of the Logistic Regression is in probabilities between 0 and 1. Instead of the slope co-efficient(w) being the rate of change of the p as X changes, now the slope co-efficient is interpreted as the rate of change of the “log odds” or \"logit function\" as X changes.\n",
        "\n",
        "#### Logit Function \n",
        "\n",
        "Logistic regression can be expressed as:\n",
        "\n",
        "$log (\\frac{p}{1 - p}) = w.X + b$\n",
        "\n",
        "where, the left hand side is called the logit or log-odds function, and $\\frac{p}{1 - p}$ is called odds.\n",
        "\n",
        "The odds signifies the ratio of probability of success to probability of failure. Therefore, in Logistic Regression, linear combination of inputs are mapped to the log(odds) - the output being equal to 1.\n",
        "\n",
        "If we take an inverse of the above function, we get:\n",
        "\n",
        "$p(X) = \\frac{e^{w.X + b}}{1 +e^{w.X + b}}$\n",
        "\n",
        "This is known as the Sigmoid function and it gives an S-shaped curve as shown above. It always gives a value of probability ranging from 0 < p < 1.\n",
        "\n",
        "#### Cost Function of the Logistic Regression\n",
        "\n",
        "The cost function consists of parameters/weights, when we say we want to optimize a cost function by this we simply refer to finding the best values of the parameters/weights.\n",
        "\n",
        "Instead of Mean Squared Error, we use a cost function called Binary Cross-Entropy, also known as Log Loss. Binary Cross-entropy loss can be divided into two separate cost functions: one for 𝑦=1 and one for 𝑦=0, that is, when the hypothesis function predicts whether the patient has heart disease or not. The Cost function is defined as follows:\n",
        "\n",
        "\n",
        "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} Cost(h_\\theta(x^{(i)}), y^{(i)}) $\n",
        "\n",
        "\n",
        "$ Cost(h_\\theta(x), y) = -log(h_\\theta(x)) $      if $y = 1$\n",
        "\n",
        "$ Cost(h_\\theta(x), y) = -log(1 - h_\\theta(x)) $      if $y = 0$\n",
        "\n",
        "\n",
        "Our task now is to choose the best parameters `w` in the above function, given the current training set, in order to minimize errors. \n",
        "\n",
        "The procedure is similar to what we did for linear regression: define a cost function and try to find the best possible values of each `w` by minimizing the cost function output. The minimization will be performed by a gradient descent algorithm, whose task is to parse the cost function output until it finds the lowest minimum point.\n",
        "\n",
        "**Above functions compressed into one**\n",
        "\n",
        "$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - (h_\\theta(x^{(i)}))] $\n",
        "\n",
        "Multiplying by $𝑦$ and $(1−𝑦)$ in the above equation is tricky, let’s us use the same equation to solve for both $y=1$ and $y=0$ cases. If $y=0$, the first side cancels out. If $y=1$, the second side cancels out. In both cases we only perform the operation we need to perform.\n",
        "\n",
        "\n",
        "**Vectorized cost function**\n",
        "\n",
        "$ h = sigmoid(w.X +b) $\n",
        "\n",
        "$J(\\theta) =  \\frac{1}{m} . [-y^{T} log(h) - (1 - y)^{T} log (1-h)]$\n",
        "\n",
        "You must be wondering why there is a negative(-) sign in the cost function,\n",
        "if you see,the values present in the log will be probabilities between 0 and 1, so,the value of log1 is 0 and the value of log0 is negative(-) infinity.\n",
        "So,the values from the cost function will always be in negative terms and that is why we add negative(-) sign to it.\n",
        "\n",
        "#### Gradient Descent\n",
        "\n",
        "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function (cost). It is just the derivative of the loss function with respect to its weights.\n",
        "\n",
        "We get the below equation after finding the derivative of the loss function:\n",
        "\n",
        "$\\frac{\\partial J(\\theta)}{\\partial \\theta} =  \\frac{1}{m} . (h - y) X^{T} $\n",
        "\n",
        "\n",
        "**Note:** To minimize the cost function we use Gradient Descent refer to the following [link](https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17) for the derivation.\n",
        "\n",
        "**Note:** Refer to the folowing link for understanding the mathematical equations of logistic regression [link](https://medium.com/@cdabakoglu/heart-disease-logistic-regression-machine-learning-d0ebf08e55c0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYydQX1t-Gb"
      },
      "source": [
        "#### Implementation of Logistic Regression\n",
        "\n",
        "For more details of the logstic regression refer to the following [link](https://medium.com/analytics-vidhya/understanding-logistic-regression-b3c672deac04)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRJTI5_3Ffg5"
      },
      "source": [
        "# Intialize the weights for the given data\n",
        "intercept = np.ones((X_train.shape[0], 1))  # Total number of samples\n",
        "x_train = np.concatenate((intercept, X_train), axis=1)\n",
        "weight = np.zeros(x_train.shape[1]) # Total number of features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Utx3koxFl6V"
      },
      "source": [
        "weight.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqYTQqhOQe4G"
      },
      "source": [
        "# Define Sigmoid Function\n",
        "def sigmoid(x, weight):\n",
        "    z = np.dot(x, weight)\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8AMR-DHC4wS"
      },
      "source": [
        "# Function to calculate the Binary cross entropy loss\n",
        "def cost_function(y_hat, y):\n",
        "    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DE2067jF29o"
      },
      "source": [
        "# Iterate over training data to update the weights using gradient descent\n",
        "# lr -> learning rate\n",
        "def fit(x, y, weight, lr, iterations):\n",
        "    for i in range(iterations):\n",
        "        \n",
        "        # Apply the sigmoid function \n",
        "        y_hat = sigmoid(x, weight)\n",
        "\n",
        "        # Compute the loss \n",
        "        loss = cost_function(y_hat, y)\n",
        "        \n",
        "        # Gradient Calculation\n",
        "        # The Gradient descent is just the derivative of the cost function with respect to its weights\n",
        "        dW = np.dot(x.T, (y_hat - y)) / y.shape[0]\n",
        "\n",
        "        # The weights are updated by subtracting the derivative (gradient descent) times the learning rate\n",
        "        # Gradient gives us the direction of the steepest ascent of the cost function and the direction of steepest descent \n",
        "        # is opposite to the gradient and that is why we substract the gradient from the weights\n",
        "        # Updating the weights\n",
        "        weight -= lr * dW\n",
        "\n",
        "    return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9LevYTLGIuc"
      },
      "source": [
        "# Starting the timer\n",
        "t0 = time()\n",
        "\n",
        "# Call the 'fit' function\n",
        "# The learning rate is usually a small number to control how fast or slow we want to move towards minimization\n",
        "updated_weights = fit(x_train, y_train, weight, 0.1, 500)\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cL3dXklGTka"
      },
      "source": [
        "# Function to predict the class label using test data\n",
        "def predict(x_new , weight, threshold):\n",
        "    x_new = np.concatenate((np.ones((x_new.shape[0], 1)), x_new), axis=1)  \n",
        "    result = sigmoid(x_new, weight)\n",
        "    result = result >= threshold\n",
        "    y_predictions = np.zeros(result.shape[0])\n",
        "    for i in range(len(y_predictions)):\n",
        "        if result[i] == True: \n",
        "            y_predictions[i] = 1\n",
        "        else:\n",
        "            continue\n",
        "    return y_predictions "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIJ5TjLUGa7w"
      },
      "source": [
        "# Call the function by passing the test data and updated weights\n",
        "y_predictions  = predict(X_test, updated_weights, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGY_iP3WGhbW"
      },
      "source": [
        "# Accuracy on test data\n",
        "(y_predictions  == y_test).sum() / len(y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbJC__VwQgjj"
      },
      "source": [
        "### Apply Logistic Regression from sklearn\n",
        "\n",
        "Refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for Logistic Regression from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxRi2fPsYlPn"
      },
      "source": [
        "# Create an instance for logistic regression\n",
        "log_reg = LogisticRegression(max_iter=500, random_state=1)\n",
        "\n",
        "# Starting the timer\n",
        "t0 = time()\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IzNXmQtIo1Y"
      },
      "source": [
        "# Get the predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIV0q8wLJYm0"
      },
      "source": [
        "# For retrieving the slope use 'log_reg.coef_'\n",
        "print('Coefficients: ', log_reg.coef_)\n",
        "\n",
        "# To retrieve the intercept\n",
        "print('Intercept: ', log_reg.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQAr-kQZAvy"
      },
      "source": [
        "# Storing the actuals and predictions in a dictionary to see the results\n",
        "actual = []\n",
        "predcition = []\n",
        "\n",
        "for i,j in zip(y_test, y_pred):\n",
        "  actual.append(i)\n",
        "  predcition.append(j)\n",
        "  \n",
        "dic = {'Actual':actual,\n",
        "       'Prediction':predcition\n",
        "       }\n",
        "result  = pd.DataFrame(dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SivZiG4OZTMe"
      },
      "source": [
        "result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hh2rvUFZY_9"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjNThs1ZcQ1"
      },
      "source": [
        "# Calculate the accuracy\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "print('True negatives: ', tn, '\\nFalse positives: ', fp, '\\nFalse negatives: ', fn, '\\nTrue positives: ', tp)"
      ],
      "metadata": {
        "id": "dT1FE058uHmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiBPCoDkZ_1O"
      },
      "source": [
        "# Print the classification report\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAV45N4KaFm8"
      },
      "source": [
        "# Confusion matrix\n",
        "sns.heatmap(confusion_matrix(y_test,y_pred), annot=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4MWqbUvnPpO"
      },
      "source": [
        "### Problem Statement: Multi-Layer Perceptron (MLP) Implementation using Keras on MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJ-Ng1Ywp_X"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "\n",
        "\n",
        "#### Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n",
        "2. Each image is Size Normalized and Centered \n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "\n",
        "#### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Q5lFwLoFZy"
      },
      "source": [
        "### Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afvcId2GMW2t"
      },
      "source": [
        "# From keras datasets download mnist images dataset\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ3JC110oMkC"
      },
      "source": [
        "Here dataset is loaded and divided into train and test images and corresponding labels. MNIST comes with 70,000 data samples with 60,000 being training data and 10,000 being test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8HKY_s3oQiQ"
      },
      "source": [
        "# Check for the shape of images and labels\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX9VV9V4oeCi"
      },
      "source": [
        "### Visualize the MNIST Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtEhgeV8obzz"
      },
      "source": [
        "fig,axes = plt.subplots(3,3,figsize=(8,12))\n",
        "ax = axes.ravel()\n",
        "\n",
        "for i in range(9):\n",
        "    ax[i].imshow(train_images[i].reshape(28,28), cmap ='gray')\n",
        "    ax[i].title.set_text('Class: ' + str(train_labels[i])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZoz49Dvp9T8"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIA3ELurUUV"
      },
      "source": [
        "# Each image in the MNIST dataset is represented as a 28x28x1 (height x width x gray scale image) image\n",
        "# but in order to apply a neural network we must\n",
        "# first \"flatten\" the image to be a list of 28x28=784 pixels\n",
        "# Reshape the train and test images\n",
        "train_images = train_images.reshape((train_images.shape[0], 28 * 28 * 1)) # 60000, 784\n",
        "test_images = test_images.reshape((test_images.shape[0], 28 * 28 * 1))    # 10000, 784"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALDfwFwqu72"
      },
      "source": [
        "# Check for the shape after flattening the images\n",
        "train_images.shape, test_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr_MQrE6sIAm"
      },
      "source": [
        "# Scale the pixels of images to the range of [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fruPWvxGqyIi"
      },
      "source": [
        "# 'to_categorical' converts a class vector (integers) to binary class matrix. E.g. for use with categorical_crossentropy. \n",
        "# y (labels): class vector to be converted into a matrix (integers from 0 to num_classes) to one-hot vector.\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "test_labels = np_utils.to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyo6tdeAzR03"
      },
      "source": [
        "Each pixel in the MNIST dataset has an integer label in the range [0, 9], one for each of the possible ten digits in the MNIST dataset. A label with a value of 0 indicates that the corresponding image contains a zero digit. Similarly, a label with a value of 8 indicates that the corresponding image contains the number eight.\n",
        "\n",
        "However, we first need to transform these integer labels into vector labels, where the index in the vector for label is set to 1 and 0 otherwise (this process is called one-hot encoding).\n",
        "\n",
        "For example, consider the label 3 and we want to binarize/one-hot encode it — the label 3 now becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`. Notice how only the index for the digit three is set to one — all other entries in the vector are set to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddM18-eCyIz2"
      },
      "source": [
        "### The Multilayer Perceptron and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-qja66yQo9"
      },
      "source": [
        "An MLP is composed of \n",
        "\n",
        "* one (passthrough) **input layer**, \n",
        "* one or more layers **hidden layers**, and \n",
        "* one final layer called the **output layer** as shown in the figure below. \n",
        "\n",
        "The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a **bias neuron** and is fully connected to the next layer.\n",
        "<br><br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1007.png\" width= 600px/>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "MLPs are trained using **backpropagation training algorithm**.\n",
        "\n",
        "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. \n",
        "\n",
        "In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
        "\n",
        "Let’s run through this algorithm in detail:\n",
        "\n",
        "* It handles one mini-batch at a time (say, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an **epoch**.\n",
        "\n",
        "* Each mini-batch is passed to the network’s **input layer**, which sends it to the first **hidden layer**. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the **output layer**. This is the **forward pass**: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
        "\n",
        "* Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
        "\n",
        "* Then it computes how much each output connection contributed to the error.\n",
        "This is done analytically by applying the chain rule, which makes this step fast and precise.\n",
        "\n",
        "* The algorithm then measures how much of these error contributions came from\n",
        "each connection in the layer below, again using the chain rule, working backward\n",
        "until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
        "\n",
        "* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
        "\n",
        "Let's summarize this algorithm again: for each training instance, the backpropagation algorithm first makes a prediction (**forward pass**) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (**reverse pass**), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n",
        "\n",
        "In order for this algorithm to work properly, the step function was replaced with an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1-Qn_6F0rtb"
      },
      "source": [
        "### Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwDHoCd0tAE"
      },
      "source": [
        "Some of the activation functions are shown below:\n",
        "\n",
        "* **Logistic (sigmoid) function:**\n",
        "\n",
        "$$σ(z) = \\frac{1}{1 + exp(–z)}$$\n",
        "\n",
        "It is an S-shaped function, exists between $0$ to $1$. Therefore, it is especially used for models where we have to predict the probability as an output. The function is differentiable.\n",
        "That means, we can find the slope of the sigmoid curve at any two points.\n",
        "\n",
        "* **Hyperbolic tangent function:** \n",
        "\n",
        "$$tanh(z) = 2σ(2z) – 1 = \\frac{2}{1 + exp(–2z)} - 1$$\n",
        "\n",
        "Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from $–1$ to $1$. That range tends to make each layer’s output more or less centered around $0$ at the beginning of training, which often helps speed up convergence.\n",
        "\n",
        "* **Rectified Linear Unit function:**\n",
        "\n",
        "$$ReLU(z) = max(0, z)$$\n",
        "\n",
        "The ReLU function is continuous but unfortunately not differentiable at $z = 0$\n",
        "(the slope changes abruptly, which can make Gradient Descent bounce around),\n",
        "and its derivative is $0$ for $z < 0$. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
        "\n",
        "These popular activation functions and their derivatives are represented in\n",
        "the figure below. \n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1008.png\" width=800px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Why do we need activation functions?** \n",
        "\n",
        "If we chain several linear transformations, all we get is a linear transformation. For example, if $f(x) = 2x + 3$ and $g(x) = 5x – 1$, then chaining these two linear functions gives you another linear function: $f(g(x)) = 2(5x – 1) + 3 = 10x + 1.$ \n",
        "\n",
        "So if we don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and we can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xumlwPO12OX_"
      },
      "source": [
        "### MLP Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW-vaO362PyH"
      },
      "source": [
        "MLPs can be used for classification and regression tasks. In classification, they can perform (i) Binary Classification (ii) Multilabel Binary Classification, and (iii) Multiclass classification\n",
        "\n",
        "* **Binary classification:** Used when there are only two distinct classes and the data we want to classify belongs exclusively to one of those classes, e.g. classifying if a review sentiment is positive or negative.\n",
        "\n",
        "* **Multilabel binary classification:** Used when there are two or more classes and the data we want to classify belongs to none of the classes or all of them at the same time, e.g. classifying which traffic signs are shown in an image.\n",
        "\n",
        "  Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels\n",
        "\n",
        "* **Multiclass classification:** Used when there are three or more classes and the data we want to classify belongs exclusively to one of those classes, e.g.  out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), we need to have one output neuron per class, and we should use the **softmax activation function** for the whole output layer as shown in the figure below. The softmax function will ensure that all the estimated probabilities are between $0$ and $1$ and that they add up to $1$.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1009.png\" width=600px/>\n",
        "</center>\n",
        "\n",
        "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn1OPbo32qvM"
      },
      "source": [
        "### Building an Image Classifier Using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1byy_Tk6gMp"
      },
      "source": [
        "\n",
        "\n",
        "The Sequential function initializes a linear stack of layers which allows to add more layers later using the Dense module.\n",
        "\n",
        "**Dense** layers are \"fully connected\" layers. In a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\n",
        "\n",
        "\n",
        "**Sequential model** is the easiest way to build a model in Keras. It allows us to build a model layer by layer. Each layer has weights that correspond to the layer that follows it. This is called the Sequential API.  We use the ‘add()’ function to add layers to our model. We will add two layers and an output layer.\n",
        "\n",
        "For more details on Sequential API refer to the following [Documentation](https://keras.io/models/sequential/\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voL_-8zd6Voe"
      },
      "source": [
        "from keras.layers import Dense, Flatten # Dense layers are \"fully connected\" layers\n",
        "from keras.models import Sequential # Importing sequential model\n",
        "\n",
        "# Flatten the image dimension to pass through the network\n",
        "image_size = 784 # 28*28\n",
        "\n",
        "# Create an instance for the sequential model\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOMnqD2x4vfB"
      },
      "source": [
        "#### Building the Neural network using Sequential API\n",
        "\n",
        "Here is a classification MLP with two hidden layers:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhGBvZVi7UzB"
      },
      "source": [
        "# Create model with 2 hidden layers and one output layer\n",
        "\n",
        "model.add(Dense(256, activation='relu', input_shape=(image_size,))) # first fully connected layer in the network\n",
        "\n",
        "model.add(Dense(128, activation=\"relu\")) # second fully connected layer in the network\n",
        "\n",
        "# The last layer is the output layer. It only has one node, which is for our prediction.\n",
        "# Output layer - 10 output classes,  softmax activation to obtain the normalized class probabilities for each prediction.\n",
        "model.add(Dense(10, activation='softmax')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GNcpB5h5rE8"
      },
      "source": [
        "Let’s go through the above code line by line:\n",
        "\n",
        "\n",
        "* We first Flatten the image dimension whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). We can specify it using the 'input_shape' parameter while adding the first Dense hidden layer with 256 neurons.\n",
        "\n",
        "* Next, it will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes $h_{W, b}(X) = ϕ( XW + b)$.\n",
        "\n",
        "* Then we add a second Dense hidden layer with 128 neurons, also using the ReLU\n",
        "activation function.\n",
        "\n",
        "* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive). Softmax is a sigmoid function applied to an independent variable with more than two categories.\n",
        "\n",
        "* Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which has a higher probability.\n",
        "\n",
        "Instead of adding the layers one by one we can pass a list of layers when creating the Sequential model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfmXSqDV3hG6"
      },
      "source": [
        "# Summary of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjPh9rX16YeZ"
      },
      "source": [
        "The model’s `summary()` method displays all the model’s layers, including each layer’s name, its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV5CrsfyOlTT"
      },
      "source": [
        "#### Compiling the model\n",
        " \n",
        "After a model is created, we must call its compile() method to specify the loss function and the optimizer to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRKnKO-g9MRg"
      },
      "source": [
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"sgd\",  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh57ACKK7L2r"
      },
      "source": [
        "In the above code cell, \n",
        "\n",
        "* first, we use the **\"categorical_crossentropy\"** loss because we have more than two label classes. Here, we have one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3). If we were doing binary classification (with one or more binary labels), then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n",
        "\n",
        "* Regarding the optimizer, **\"sgd\"** means that we will train the model using simple Stochastic Gradient Descent. \n",
        "\n",
        "* Finally, since this is a classifier, it’s useful to measure its **\"accuracy\"** during training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hov8hLS59eBn"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3knzaM9bVl"
      },
      "source": [
        "# Training model on Training set\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgcTEOBI9rus"
      },
      "source": [
        "The `fit()` method returns a History object containing the training parameters\n",
        "(`history.params`), the list of epochs it went through (`history.epoch`), and most importantly a dictionary (`history.history`) containing the loss and extra metrics it measured at the end of each epoch on the training set. Keras does backpropagation automatically by using the fit method\n",
        "\n",
        "Let's plot the learning curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MRPUY7193oJ"
      },
      "source": [
        "# Visualize training and validation metrics\n",
        "df = pd.DataFrame(history.history)\n",
        "df.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "\n",
        "# Set the vertical range to [0-1]\n",
        "plt.gca().set_ylim(0, 1) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLu0EM9niNka"
      },
      "source": [
        "We can see that both the training accuracy and the validation accuracy steadily\n",
        "increase during training, while the training loss and the validation loss decrease. Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting.\n",
        "\n",
        "Once we are satisfied with the model’s validation accuracy, we should evaluate it on the test set to estimate the generalization error before we deploy the model to production. We can easily do this using the `evaluate()` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnsd3jGEH4a"
      },
      "source": [
        "#### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMxqDWtI9hQp"
      },
      "source": [
        "# Model performance on test set\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrcf0U-vgueY"
      },
      "source": [
        "Let's see how to implement back propagation algorithm from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJt-uGJvymM2"
      },
      "source": [
        "### Problem Statement: Implementing the XOR Gate using Backpropagation in Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u19_H2cqaF22"
      },
      "source": [
        "####  X-OR problem in Neural Networks\n",
        "\n",
        "The XOR problem is a classic problem in artificial neural network research. It consists of predicting output value of exclusive-OR gate. We do it using a Multi layer perceptron. \n",
        "\n",
        "Minksy and Papert (1969) showed that classifying XOR was a big problem for neural network architectures of the 1960s, known as perceptrons. Let’s looks at how a multi-layer perceptron based neural network solves this problem of the truth table, called the ‘XOR’ (either A or B but not both). \n",
        "\n",
        "\n",
        "Implementing logic gates using neural networks help understand the mathematical computation by which a neural network processes its inputs to arrive at a certain output. This neural network will deal with the XOR logic problem. An XOR (exclusive OR gate) is a digital logic gate that gives a true output only when both its inputs differ from each other. The truth table for an XOR gate is shown below:\n",
        "\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR9Q-xKcyvxJ13ElQFJsBTws8coBQeZen2qpl0x0XQqkN0DnYRc)\n",
        "\n",
        "\n",
        "The goal of the neural network is to classify the input patterns according to the above truth table. If the input patterns are plotted according to their outputs, it is seen that these points are not linearly separable. Hence the neural network has to be modeled to separate these input patterns using decision planes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgiBwDDnaRqk"
      },
      "source": [
        "Here, we build a neural network which consist of one input layer with two nodes (X1, X2); one hidden layer with two nodes (since two decision planes are needed); and one output layer with one node (Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPLLaOx_aVO6"
      },
      "source": [
        "#### Define the inputs and structure of the neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeZm36ZOF5rh"
      },
      "source": [
        "# These are XOR inputs\n",
        "x = np.array([[0,0,1,1],[0,1,0,1]])\n",
        "\n",
        "# These are XOR outputs\n",
        "y = np.array([[0,1,1,0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjGampQpaFK_"
      },
      "source": [
        "# Number of inputs\n",
        "n_x = 2\n",
        "\n",
        "# Number of neurons in output layer\n",
        "n_y = 1\n",
        "\n",
        "# Number of neurons in hidden layer\n",
        "n_h = 2\n",
        "\n",
        "# Total training examples\n",
        "m = x.shape[1]\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.5\n",
        "\n",
        "# Define random seed for consistent results\n",
        "np.random.seed(2)\n",
        "\n",
        "# Define weight matrices for neural network\n",
        "w1 = np.random.rand(n_h,n_x)   # Weight matrix for hidden layer\n",
        "w2 = np.random.rand(n_y,n_h)   # Weight matrix for output layer\n",
        "\n",
        "losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B-hoQtcvuac"
      },
      "source": [
        "#### Update the weights using feedforward and backpropagation algorithm\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com//DLFA/Experiment_related_data/XOR.png\" width=700px/>\n",
        "</center>\n",
        "\n",
        "For simplicity we consider '$W_{11}$', '$W_{12}$', '$W_{21}$' and '$W_{22}$' as weight vector '$W_{1}$'. Weights '$W_{3}$' and '$W_{4}$' as '$W_{2}$'\n",
        "\n",
        "Here 'Z' is the dot product of weight vector 'W' and the input vector 'X'. Again we vectorize '$Z_{11}$', '$Z_{12}$' as '$Z_{1}$' and '$Z_{2}$' remains same\n",
        "\n",
        "$'A'$ is the activation function. We used sigmoid activation function in our network. Vectorizing $'A_{11}'$ and $'A_{12}'$ gives $'A_{1}'$ and $'A_{2}'$ is kept the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik9tBgpXKmGK"
      },
      "source": [
        "#### Forward propagation\n",
        "\n",
        "In the forward propagation all 'Z' and 'A' will be calculated until we get to the end of our network giving us our prediction 'Y'.\n",
        "\n",
        "$ Z_{1} = W_1 . X + b_1$     --------> 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV60omIIKfQ5"
      },
      "source": [
        "#### Activation Function\n",
        "\n",
        "Sigmoid function is a S-shaped curve which predicts the probability as an ouput.\n",
        "\n",
        "$A_1 = \\frac{1}{1 + e ^ {- Z_1}}$   ---------> 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9dDvkKLuFa"
      },
      "source": [
        "$ Z_{2} = W_2 . A_1 + b_2$    ---------> 3\n",
        "\n",
        "Y = $A_2 = \\frac{1}{1 + e ^ {- Z_2}}$   ---------> 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwXEuxUTbNe9"
      },
      "source": [
        "# Sigmoid Function\n",
        "def sigmoid(z):\n",
        "    z = 1/(1+np.exp(-z))\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t3jbYr-GtEO"
      },
      "source": [
        "# Forward propagation\n",
        "def forward_prop(w1,w2,x):\n",
        "    z1 = np.dot(w1,x)\n",
        "    a1 = sigmoid(z1)    \n",
        "    z2 = np.dot(w2,a1)\n",
        "    a2 = sigmoid(z2)\n",
        "    return z1,a1,z2,a2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLO4uCjAyEXd"
      },
      "source": [
        "Basic Algorithm which explains feedforward and backpropagation\n",
        "\n",
        "1. Inputs are taken\n",
        "2. The weights are usually randomly selected.\n",
        "3. Calculate the output of every neuron from the input layer, to the hidden layers and to the output layer.\n",
        "4. Apply sigmoid function at hidden and output layers.\n",
        "5. Calculate the error in the outputs\n",
        "\n",
        "    *Error = Predicted Output – Actual Output*\n",
        "5. Travel back from the output layer to the hidden layer to adjust the weights such that the error is minimized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woRC6ENkhkQo"
      },
      "source": [
        "**Chain rule to find the change in error with respect to W2 (Weights):**\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = \\frac{\\partial E}{\\partial O} \\cdot \\frac{\\partial O}{\\partial Z_{2}} \\cdot \\frac{\\partial Z_{2}}{\\partial W_{2}}$\n",
        "\n",
        "* **Change in error with respect to output**\n",
        "\n",
        "  Suppose the actual output is represented as 'O' and the predicted output is represented as 'a2', then the error would be calculated as:\n",
        "\n",
        "  $E = \\frac{1}{2}\\left ( a2-O \\right )^{2}$\n",
        "\n",
        "  Differentiate the error with respect to the output\n",
        "\n",
        "  $\\frac{\\partial E}{\\partial O} = -\\left (a2-O  \\right )$\n",
        "\n",
        "   $\\frac{\\partial E}{\\partial O} = O-a2$\n",
        "\n",
        "  In the code, `predicted output` is represented as `output_layer_outputs` and the `actual output` is represented as y\n",
        "\n",
        "* **Change in output with respect to Z2** \n",
        "\n",
        "  Thus, $\\frac{\\partial O}{\\partial Z_{2}}$ is effectively the derivative of Sigmoid\n",
        "\n",
        "  $f\\left ( z \\right ) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "  $f'(z) = (1+e^{-z})-1[1-(1+e^{-z})-1]$\n",
        "\n",
        "  ${f}'\\left ( z \\right ) = sigmoid(z)(1-sigmoid(z))$\n",
        "\n",
        "  $\\frac{\\partial O}{\\partial Z_{2}} = (a2)(1-a2)$ \n",
        "\n",
        "\n",
        "* **Change in $Z_2$ with respect to $W_2$ (Weights)**\n",
        "\n",
        "  $Z_2 = W^T.A_1 $\n",
        "\n",
        "  On differentiating $Z_2$ with respect to $W_2$, we will get the value $A_1$ itself\n",
        "\n",
        "  $\\frac{\\partial Z_{2}}{\\partial W_{2} }$ = $A_1$ where $A_1$ = $sigmoid(Z_{1})$\n",
        "\n",
        "**Chain rule to find the change in error with respect to W2 (Weights):**\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = \\frac{\\partial E}{\\partial O} \\cdot \\frac{\\partial O}{\\partial Z_{2}} \\cdot \\frac{\\partial Z_{2}}{\\partial W_{2}}$\n",
        "\n",
        "$\\frac{\\partial E}{\\partial W_{2}} = (a2-y) . a2(1-a2). A_1$\n",
        "\n",
        "In the code $A_1$ is represented as $a_1$\n",
        "\n",
        "**Update the weights using Gradient Descent**\n",
        "\n",
        "$W_{new} = W_{old} - lr * \\frac{\\partial E}{\\partial W_{2}} $\n",
        "\n",
        "In the code, $W_{old}$ is represented as $w_2$,  $lr$ is represented as learning rate and $\\frac{\\partial E}{\\partial W}$ is represented as $W_2$.\n",
        "\n",
        "Similarly, W1 (Weights) are updated using chain rule.\n",
        "\n",
        "**Note:** For more details refer to the following [link](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC_1em2QZ6aj"
      },
      "source": [
        "# Backward propagation\n",
        "def back_prop(w1,w2,z1,a1,z2,a2,y):\n",
        "\n",
        "    # Updating the weights with respect to w2 (hidden weights)\n",
        "\n",
        "    dE = a2 - y      # ∂E/∂O\n",
        "    dO = a2 * (1 - a2)    # ∂O/∂Z2\n",
        "    dz2 = dE * dO          # ∂E/∂O * ∂O/∂Z2\n",
        "    dw2 = np.dot(dz2, a1.T)  # ∂E/∂W2 = ∂E/∂O * ∂O/∂Z2 * ∂Z2/∂W2\n",
        "    \n",
        "    # Updating the weights with respect to w1 (input weights)\n",
        "\n",
        "    dz1 = np.dot(w2.T,dz2) * a1*(1-a1)\n",
        "    dw1 = np.dot(dz1,x.T)\n",
        "        \n",
        "    return dz2, dw2, dz1, dw1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FRKJCn5aXpv"
      },
      "source": [
        "# Training the network\n",
        "iterations = 10000\n",
        "for i in range(iterations):\n",
        "\n",
        "    # Call the forward propagation function\n",
        "    z1, a1, z2, a2 = forward_prop(w1, w2, x)\n",
        "\n",
        "    # Binary cross entropy loss \n",
        "    loss = -(1/m) * np.sum(y * np.log(a2) + (1 - y) * np.log(1 - a2))\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Call the Backward propagation function\n",
        "    dz2, dw2, dz1, dw1 = back_prop(w1, w2, z1, a1, z2, a2, y)\n",
        "    \n",
        "    # Updating weights using gradient descent\n",
        "    w2 = w2 - lr * dw2\n",
        "    w1 = w1 - lr * dw1\n",
        "\n",
        "# We plot losses to see how our network is doing\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"EPOCHS\")\n",
        "plt.ylabel(\"Loss value\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEvQxsVtcw9Z"
      },
      "source": [
        "Define a predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q780Uw3cdfY"
      },
      "source": [
        "def predict(w1,w2,input):\n",
        "    z1,a1,z2,a2 = forward_prop(w1,w2,test)\n",
        "    # Set the threshold for the predicted output\n",
        "    if a2 >= 0.5:\n",
        "        print(\"For input\", [i[0] for i in input], \"output is 1\")\n",
        "    else:\n",
        "        print(\"For input\", [i[0] for i in input], \"output is 0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HQIyb3FcuKc"
      },
      "source": [
        "Here are the predictions of our trained neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rkf6P4Qcp7U"
      },
      "source": [
        "test = np.array([[1],[0]])\n",
        "predict(w1,w2,test)\n",
        "test = np.array([[0],[0]])\n",
        "predict(w1,w2,test)\n",
        "test = np.array([[0],[1]])\n",
        "predict(w1,w2,test)\n",
        "test = np.array([[1],[1]])\n",
        "predict(w1,w2,test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zlW2FdY5_JK"
      },
      "source": [
        "Our goal with backpropagation is to update each of the weights in the network so that predicted output will be closer to the actual output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Suppose the network has 784 inputs, 32 neurons in the first hidden layer, 16 neurons in the second hidden layer, and 10 neurons in the output layer. How many trainable parameters does the network have? Assume there is a bias at every output layer.\n",
        "Answer1 = \"\" #@param [\"\",\"25248\", \"25600\",\"672\",\"25818\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRvN7yU-e18h"
      },
      "source": [
        "#### Consider the below figure and answer Q.2.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Picture.png\" width=400px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. You are given the above neural networks which take two binary valued inputs x1, x2 ∈ {0, 1} and the activation function is the threshold function (h(x) = 1 if x > 0; 0 otherwise). Which of the following logical functions does it compute?\n",
        "Answer2 = \"\" #@param [\"\",\"AND\", \"OR\",\" NAND\",\"XOR\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}